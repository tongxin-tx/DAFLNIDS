{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# coding=utf-8\n",
    "import csv\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import logging\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Reshape, Dropout, Conv1D, MaxPooling1D, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "np.set_printoptions(threshold=10000000000000)\n",
    "\n",
    "\n",
    "def load_traindata(path):\n",
    "    file_path = path\n",
    "    feature = []\n",
    "    label = []\n",
    "    with (open(file_path, 'r')) as data_from:\n",
    "        csv_reader = csv.reader(data_from)\n",
    "        for i in csv_reader:\n",
    "            t = i[:78]\n",
    "            t.append(0)\n",
    "            t.append(0)\n",
    "\n",
    "            if int(i[78]) == 0:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "\n",
    "            elif int(i[78]) == 1:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "            elif int(i[78]) == 2:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "            elif int(i[78]) == 3:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "            elif int(i[78]) == 4:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "            elif int(i[78]) == 5:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "            elif int(i[78]) == 6:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "    return feature, label\n",
    "\n",
    "def load_testdata():\n",
    "    file_path = 'CICIDS2018Test/mtest.csv'\n",
    "    feature = []\n",
    "    label = []\n",
    "    with (open(file_path, 'r')) as data_from:\n",
    "        csv_reader = csv.reader(data_from)\n",
    "        for i in csv_reader:\n",
    "            t = i[:78]\n",
    "            t.append(0)\n",
    "            t.append(0)\n",
    "\n",
    "            if int(i[78]) == 0:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "\n",
    "            elif int(i[78]) == 1:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "            elif int(i[78]) == 2:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "            elif int(i[78]) == 3:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "            elif int(i[78]) == 4:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "            elif int(i[78]) == 5:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "            elif int(i[78]) == 6:\n",
    "                feature.append(t)\n",
    "                label_list = [0] * 7\n",
    "                label_list[int(i[78])] = 1\n",
    "                label.append(label_list)\n",
    "    return feature, label\n",
    "\n",
    "def split_client_dataset(num_clients: int, len_dataset: int, fixed_size: int=None):\n",
    "    \"\"\"Generate a index list based on number of clients and length of dataset.\n",
    "      Args:\n",
    "          num_clients: number of clients\n",
    "          len_dataset: Number of samples in the whole dataset\n",
    "          fixed_size: If setted, each client will only take a fixed number of samples.\n",
    "      Returns:\n",
    "          A nested list with index list for each client.\n",
    "    \"\"\"\n",
    "    ind_list = np.linspace(0, len_dataset - 1, len_dataset).astype(np.int32)\n",
    "    client_data_list = []\n",
    "    size = int(len_dataset / num_clients)\n",
    "    for client in range(num_clients):\n",
    "        if fixed_size is not None:\n",
    "            size = fixed_size\n",
    "        data_list = np.random.choice(ind_list, size, replace=False)\n",
    "\n",
    "        ind_list = [i for i in ind_list if i not in data_list]\n",
    "        client_data_list.append(data_list)\n",
    "    return client_data_list\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Get a CNN model in keras.\"\"\"\n",
    "    model = Sequential()\n",
    "    # model.add(Reshape((-1,6, 6,1)))\n",
    "    model.add(Conv2D(32, kernel_size=(4, 5), strides=(1, 1),\n",
    "                     input_shape=(8, 10, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(7))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def init(model,x_test,y_test):\n",
    "    \"\"\"In keras if you don't run a funcition of the model, the model's wight would be empty [0].\n",
    "       This is only for weight initilization.\n",
    "    \"\"\"\n",
    "    model.evaluate(x_test[0:1,...], y_test[0:1,...],verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    x_train, y_train = load_traindata('CICIDS2018Train/train.csv')\n",
    "    x_test, y_test =load_testdata()\n",
    "\n",
    "\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    x_train = min_max_scaler.fit_transform(x_train)\n",
    "    x_test = min_max_scaler.fit_transform(x_test)\n",
    "\n",
    "\n",
    "    x_train = np.array(x_train)\n",
    "    x_test = np.array(x_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    t_round = 20\n",
    "    numclients=2\n",
    "    num_clients = 3\n",
    "    len_dataset = x_train.shape[0]\n",
    "    print(x_train.shape[0])\n",
    "    print(x_train[0])\n",
    "    #print(len(x_train[0]))\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], 8, 10, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 8, 10, 1)\n",
    "\n",
    "    global_model = get_model()\n",
    "    print(global_model.summary(line_length=150, positions=[0.30, 0.60, 0.7, 1.]))\n",
    "\n",
    "    print(global_model.get_weights()[0].shape)\n",
    "    print(global_model.get_weights()[1].shape)\n",
    "    print(global_model.get_weights()[2].shape)\n",
    "    print(global_model.get_weights()[3].shape)\n",
    "    print(global_model.get_weights()[4].shape)\n",
    "    print(global_model.get_weights()[5].shape)\n",
    "    init(global_model, x_test, y_test)\n",
    "\n",
    "    acc_list = []\n",
    "    acc1 = []\n",
    "    acc2 = []\n",
    "    acc3 = []\n",
    "    acc4 = []\n",
    "    acc5 = []\n",
    "    acc6=[]\n",
    "    acc7=[]\n",
    "\n",
    "    #分割数据集\n",
    "    client_data_list = split_client_dataset(2, len_dataset, fixed_size=None)\n",
    "\n",
    "    x_train1, y_train1 = load_traindata('CICIDS2018Train/preDDos.csv')\n",
    "    min_max_scaler1 = MinMaxScaler()\n",
    "    x_train1 = min_max_scaler.fit_transform(x_train1)\n",
    "    x_train1 = np.array(x_train1)\n",
    "    y_train1 = np.array(y_train1)\n",
    "    x_train1 = x_train1.reshape(x_train1.shape[0], 8, 10, 1)\n",
    "\n",
    "    # x_train2, y_train2 = load_traindata('CICIDS2018Train/preInfilteration.csv')\n",
    "    # x_train2 = min_max_scaler.fit_transform(x_train2)\n",
    "    # x_train2 = np.array(x_train2, dtype=object)\n",
    "    # y_train2 = np.array(y_train2, dtype=object)\n",
    "    # x_train2 = x_train2.reshape(x_train2.shape[0], 8, 10, 1)\n",
    "    #\n",
    "    # x_train3, y_train3 = load_traindata('CICIDS2018Train/preBenign.csv')\n",
    "    # x_train3 = min_max_scaler.fit_transform(x_train3)\n",
    "    # x_train3 = np.array(x_train3, dtype=object)\n",
    "    # y_train3 = np.array(y_train3, dtype=object)\n",
    "    # x_train3 = x_train3.reshape(x_train3.shape[0], 8, 10, 1)\n",
    "\n",
    "\n",
    "\n",
    "    for r in range(t_round):\n",
    "        print(\"Round: \"+str(r+1)+\" started.\")\n",
    "\n",
    "        acc=[]\n",
    "        # Size of weight based on the model\n",
    "        weight_acc = np.asarray([np.zeros((4, 5, 1, 32)), np.zeros(\n",
    "            (32,)), np.zeros((384, 1024)), np.zeros((1024,)), np.zeros((1024, 7))\n",
    "                                    , np.zeros((7,))\n",
    "                                 ])\n",
    "        sumw=[]\n",
    "        numw = []\n",
    "        for c in range(num_clients):\n",
    "            model = get_model()\n",
    "            init(model,x_test,y_test)\n",
    "            model.set_weights(global_model.get_weights())\n",
    "\n",
    "            if c==2:\n",
    "\n",
    "                c_feature=x_train1\n",
    "                c_label=y_train1\n",
    "            # elif c==5:\n",
    "            #     c_feature = x_train2\n",
    "            #     c_label = y_train2\n",
    "            # elif c==6:\n",
    "            #     c_feature = x_train3\n",
    "            #     c_label = y_train3\n",
    "            elif c<=1:\n",
    "                ind = client_data_list[c]\n",
    "                c_feature = np.take(x_train, ind, axis=0)\n",
    "                c_label = np.take(y_train, ind, axis=0)\n",
    "\n",
    "            numw.append(len(c_feature))\n",
    "            # Train client\n",
    "            model.fit(x=c_feature, y=c_label, epochs=1, validation_split=0.1, batch_size=512, verbose=1)  # cnn\n",
    "\n",
    "            param_after = np.asarray(model.get_weights())\n",
    "\n",
    "            sumw.append(param_after)\n",
    "\n",
    "            score = model.evaluate(x_test, y_test, verbose=0)\n",
    "            print('Client: '+str(c+1)+' with accuracy:', score[1])\n",
    "            if c==0: acc1.append(score[1])\n",
    "            elif c==1: acc2.append(score[1])\n",
    "            elif c == 2:acc3.append(score[1])\n",
    "            elif c == 3: acc4.append(score[1])\n",
    "            elif c == 4: acc5.append(score[1])\n",
    "            elif c == 5: acc6.append(score[1])\n",
    "            elif c == 6: acc7.append(score[1])\n",
    "            acc.append(score[1])\n",
    "\n",
    "        nacc=[]\n",
    "        nsumw=[]\n",
    "\n",
    "        for c2 in range(num_clients):\n",
    "            if acc[c2]>=0.75:\n",
    "                nacc.append(np.exp(acc[c2]))\n",
    "                nsumw.append(sumw[c2])\n",
    "\n",
    "\n",
    "        weight = []\n",
    "        accw=[]\n",
    "        for c1 in range(len(nacc)):\n",
    "            print(numw[c1]/sum(numw))\n",
    "            print(nacc[c1]/sum(nacc))\n",
    "            weight.append((nacc[c1]/sum(nacc))*(numw[c1]/sum(numw)))\n",
    "\n",
    "        for c1 in range(len(nacc)):\n",
    "            print(weight[c1] / sum(weight))\n",
    "            weight_acc += nsumw[c1] *(weight[c1]/sum(weight))\n",
    "\n",
    "\n",
    "        global_model.set_weights(weight_acc)\n",
    "        score = global_model.evaluate(x_test, y_test, verbose=0)\n",
    "        print('Global test loss:', score[0])\n",
    "        print('Global accuracy:', score[1])\n",
    "        acc_list.append(score[1])\n",
    "\n",
    "    preds = global_model.predict(x_test)\n",
    "    pred_lbls = np.argmax(preds,axis=1)\n",
    "    true_lbls = np.argmax(y_test,axis=1)\n",
    "    #print(pic_client)\n",
    "    confusion_matrix(true_lbls, pred_lbls)\n",
    "\n",
    "    print(\"confusion_matrix\",confusion_matrix(true_lbls, pred_lbls))\n",
    "\n",
    "    from sklearn.metrics import f1_score,recall_score,accuracy_score,precision_score\n",
    "    sumf1=f1_score(true_lbls, pred_lbls, average='weighted')\n",
    "    print(\"***********\")\n",
    "    print('f1',sumf1)\n",
    "    print('recall',recall_score(true_lbls, pred_lbls, average='weighted'))\n",
    "    print('accuracy',accuracy_score(true_lbls, pred_lbls))\n",
    "    print('precision',precision_score(true_lbls, pred_lbls, average='weighted'))\n",
    "\n",
    "\n",
    "\n",
    "    from sklearn.metrics import multilabel_confusion_matrix\n",
    "    conf = multilabel_confusion_matrix(true_lbls, pred_lbls)\n",
    "\n",
    "    print(\"***********\")\n",
    "    print(\"confusion_matrix\",conf)\n",
    "    class1=['Normal','Infilteration','Bot','DoS','SQL Injection','Brute Force','DDos']\n",
    "    for i in range(conf.shape[0]):\n",
    "\n",
    "        tn = conf[i][0][0]\n",
    "        fp = conf[i][0][1]\n",
    "        fn = conf[i][1][0]\n",
    "        tp = conf[i][1][1]\n",
    "        acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        fpr = fp/(fp+tn)\n",
    "        tpr = tp/(tp+fn)\n",
    "\n",
    "        Precision=tp/(tp+fp)\n",
    "        f1=(2*Precision*tpr)/(Precision+tpr)\n",
    "        #print(conf[i])\n",
    "        print(class1[i])\n",
    "        print(\"Accuracy\",float(\"{0:5f}\".format(acc))*100)\n",
    "        print(\"Precision\",float(\"{0:5f}\".format(Precision))*100)\n",
    "        print(\"Recall\", float(\"{0:5f}\".format(tpr)) * 100)\n",
    "        print(\"f1\", float(\"{0:5f}\".format(f1)) * 100)\n",
    "\n",
    "    print(acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}